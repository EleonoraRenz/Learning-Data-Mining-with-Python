{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining CS 619, Spring 2018 - Eleonora Renz\n",
    "\n",
    "### Week 6 - Chapter 6\n",
    "\n",
    "## Social Media Insight using Naive Bayes - ch6_recreate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "tweet_filename = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"DataMining_Spring2018\", \"Data\", \"twitter\", \"replicable_python_tweets.json\")\n",
    "labels_filename = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"DataMining_Spring2018\", \"Data\", \"twitter\", \"replicable_python_classes.json\")\n",
    "\n",
    "replicable_dataset = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"DataMining_Spring2018\", \"Data\", \"twitter\", \"replicable_dataset.json\")\n",
    "\n",
    "with open(replicable_dataset) as inf:\n",
    "    tweet_ids = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store all still available labels (some may have been deleted since we collected them).\n",
    "actual_labels = []\n",
    "label_mapping = dict(tweet_ids)\n",
    "\n",
    "import twitter\n",
    "\n",
    "\n",
    "consumer_key = \"fjWS8WLO6iR6ZSi22lCt0XwJO\"\n",
    "consumer_secret = \"3dQXyQdcM21vpDklxqnYPpCFMGsdJZnEI0xG6u5jp9J6fU7cSY\"\n",
    "access_token = \"840475426162110464-dR342dwz0THqN4aI01nBCkGQ9aOJc0e\"\n",
    "access_token_secret = \"UByuQL9sTe7UhoNwqtnlJFZ75cFxsJJOmYgqYoTyoBaVv\"\n",
    "authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n",
    "\n",
    "t = twitter.Twitter(auth=authorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ids = [tweet_id for tweet_id, label in tweet_ids]\n",
    "with open(tweet_filename, 'a') as output_file:\n",
    "    # We can lookup 100 tweets at a time, which saves time in asking twitter for them\n",
    "    for start_index in range(0, len(all_ids), 100):\n",
    "        id_string = \",\".join(str(i) for i in all_ids[start_index:start_index+100])\n",
    "        search_results = t.statuses.lookup(_id=id_string)\n",
    "        for tweet in search_results:\n",
    "            if 'text' in tweet:\n",
    "                # Valid tweet - save to file\n",
    "                output_file.write(json.dumps(tweet))\n",
    "                output_file.write(\"nn\")\n",
    "                actual_labels.append(label_mapping[tweet['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(labels_filename, 'w') as outf:\n",
    "    json.dump(actual_labels, outf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

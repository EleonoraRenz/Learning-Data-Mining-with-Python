{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-word models\n",
    "\n",
    "We create a matrix, where each row represents a document in our dataset and each column\n",
    "represents a word. The value of the cell is the frequency of that word in the document. This\n",
    "is known as the bag-of-words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9), ('for', 4), ('in', 4), ('to', 4), ('one', 4)]\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"Three Rings for the Elven-kings under the sky, Seven for the Dwarflords\n",
    "in halls of stone, Nine for Mortal Men, doomed to die, One for the\n",
    "Dark Lord on his dark throne In the Land of Mordor where the Shadows lie.\n",
    "One Ring to rule them all, One Ring to find them, One Ring to bring them\n",
    "all and in the darkness bind them. In the Land of Mordor where the Shadows\n",
    "lie. \"\"\".lower()\n",
    "words = s.split()\n",
    "from collections import Counter\n",
    "c = Counter(words)\n",
    "print(c.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model has three major types, with many variations and alterations.\n",
    "<ul><li>The first is to use the raw frequencies, as shown in the preceding example. This\n",
    "has the same drawback as any non-normalised data - words with high variance\n",
    "due to high overall values (such as) the overshadow lower frequency (and\n",
    "therefore lower-variance) words, even though the presence of the word the rarely\n",
    "has much importance.</li><li>\n",
    "The second model is to use the normalized frequency, where each document's\n",
    "sum equals 1. This is a much better solution as the length of the document doesn't\n",
    "matter as much, but it still means words like the overshadow lower frequency\n",
    "words. The third type is to simply use binary featuresâ€”a value is 1 if it occurs,\n",
    "and 0 otherwise. We will use binary representation in this chapter.</li><li>\n",
    "Another (arguably more popular) method for performing normalization is called\n",
    "term frequency-inverse document frequency (tf-idf). In this weighting scheme,\n",
    "term counts are first normalized to frequencies and then divided by the number\n",
    "of documents in which it appears in the corpus. We will use tf-idf in Chapter 10,\n",
    "Clustering News Articles.</li></ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
